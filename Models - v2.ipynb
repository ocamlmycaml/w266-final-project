{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.python.ops import array_ops\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, Source, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN)\n",
    "test_data = load_pandas(TrainOrTest.TEST)\n",
    "\n",
    "print(train_data['label'].unique())\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='unk')\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['text']\n",
    "    for sent in sent_tokenize(doc)\n",
    "    for word in word_tokenize(sent)\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "vocab_size, t.word_index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word, embeddings_index['unk'])\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(t.word_index.keys() - embeddings_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = \"\"\"Dear abby,\n",
    "\n",
    "I'm writing to tell you you suck. Help me out of this mess.\n",
    "\n",
    "Bye\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tok=None):\n",
    "    return [\n",
    "        [\n",
    "            tok.texts_to_sequences(nltk.word_tokenize(sent))\n",
    "            if tok else\n",
    "            nltk.word_tokenize(sent)\n",
    "            for sent in nltk.sent_tokenize(para)\n",
    "        ]\n",
    "        for para in text.splitlines()\n",
    "        if len(para) > 0\n",
    "    ]\n",
    "\n",
    "print(tokenize(example_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(example_document, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokenized'] = train_data['text'].map(lambda text: tokenize(text, t))\n",
    "test_data['tokenized'] = test_data['text'].map(lambda text: tokenize(text, t))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 0\n",
    "MAX_PARA_LENGTH = 0\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for doc in train_data['tokenized'].append(test_data['tokenized']):\n",
    "    MAX_DOC_LENGTH = max(MAX_DOC_LENGTH, len(doc))\n",
    "    for para in doc:\n",
    "        MAX_PARA_LENGTH = max(MAX_PARA_LENGTH, len(para))\n",
    "        for sent in para:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(sent))\n",
    "            \n",
    "MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_dense(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                sentnp = np.hstack(np.array(sent))\n",
    "                Z[docidx, paraidx, sentidx, :len(sentnp)] += sentnp\n",
    "    return Z\n",
    "\n",
    "print(train_data['tokenized'][0])\n",
    "pad_to_dense(train_data['tokenized'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_mask(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len), dtype=bool)\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                for tokenidx, token in enumerate(sent):\n",
    "                    Z[docidx, paraidx, sentidx, tokenidx] = True\n",
    "    return Z\n",
    "\n",
    "dense_mask(train_data['tokenized'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = pad_to_dense(train_data['tokenized'])\n",
    "test_tensor = pad_to_dense(test_data['tokenized'])\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_labels(labels):\n",
    "    eye = [\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for item in labels:\n",
    "        result.append(eye[int(item) - 1])\n",
    "        \n",
    "    return np.array(result)\n",
    "\n",
    "train_labels = categorical_labels(pd.to_numeric(train_data['label']))\n",
    "test_labels = categorical_labels(pd.to_numeric(test_data['label']))\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent_mask = dense_mask(train_data['tokenized'])\n",
    "test_sent_mask = dense_mask(test_data['tokenized'])\n",
    "\n",
    "train_para_mask = np.apply_along_axis(any, 3, train_sent_mask)\n",
    "test_para_mask = np.apply_along_axis(any, 3, test_sent_mask)\n",
    "\n",
    "train_doc_mask = np.apply_along_axis(any, 2, train_para_mask)\n",
    "test_doc_mask = np.apply_along_axis(any, 2, test_para_mask)\n",
    "\n",
    "(train_sent_mask.shape, train_para_mask.shape, train_doc_mask.shape), (test_sent_mask.shape, test_para_mask.shape, test_doc_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((train_tensor, train_sent_mask, train_para_mask, train_doc_mask), train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((test_tensor, test_sent_mask, test_para_mask, test_doc_mask), test_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "(a, b, c, d), e = next(iter(train_dataset))\n",
    "a.shape, b.shape, c.shape, d.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False)\n",
    "\n",
    "embedded_example = embedding(train_tensor[:2])\n",
    "\n",
    "print(embedded_example.shape)\n",
    "embedded_example[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, values):\n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W(values)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001, return_sequences=True))\n",
    "        self.concat = Concatenate()\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs, mask):\n",
    "        encoded = self.lstm(inputs, mask=mask)\n",
    "        output, attention_weights = self.attention(encoded)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2  # keep it low\n",
    "EPOCHS = 1#00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        self.para_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        self.doc_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH, -1))\n",
    "        para_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH))\n",
    "        \n",
    "        para_embedded = self.para_encoder(sent_embedded, mask=para_mask)\n",
    "        para_embedded = array_ops.reshape(\n",
    "            para_embedded, (BATCH_SIZE, MAX_DOC_LENGTH, -1))\n",
    "        \n",
    "        x = self.doc_encoder(para_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "doc_model = DocModel(150, 100, 0.5)\n",
    "doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model_hist = doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.para_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded, sent_weights = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH, -1))\n",
    "        para_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH))\n",
    "        \n",
    "        para_embedded, para_weights = self.para_encoder(sent_embedded, mask=para_mask)\n",
    "        para_embedded = array_ops.reshape(\n",
    "            para_embedded, (BATCH_SIZE, MAX_DOC_LENGTH, -1))\n",
    "        \n",
    "        x, doc_weights = self.doc_encoder(para_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if not training:\n",
    "            self.sent_weights = sent_weights\n",
    "            self.para_weights = para_weights\n",
    "            self.doc_weights = doc_weights\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "attentive_doc_model = AttentiveDocModel(150, 100, 50, 0.5)\n",
    "attentive_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "attentive_doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "attentive_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attentive_doc_model_hist = attentive_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentive_doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallAttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded, sent_weights = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH, -1))\n",
    "        doc_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH))\n",
    "        \n",
    "        x, doc_weights = self.doc_encoder(sent_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if not training:\n",
    "            self.sent_weights = sent_weights\n",
    "            self.doc_weights = doc_weights\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "small_att_doc_model = SmallAttentiveDocModel(150, 100, 50, 0.5)\n",
    "small_att_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "small_att_doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "small_att_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_att_doc_model_hist = small_att_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_att_doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        self.doc_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH, -1))\n",
    "        doc_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH))\n",
    "        \n",
    "        x = self.doc_encoder(sent_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "small_doc_model = SmallDocModel(150, 100, 0.5)\n",
    "small_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "small_doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "small_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_doc_model_hist = small_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyAttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH * MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH * MAX_SENT_LENGTH))\n",
    "        \n",
    "        x, doc_weights = self.doc_encoder(embedded, mask=sent_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if not training:\n",
    "            self.doc_weights = doc_weights\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "tiny_att_doc_model = TinyAttentiveDocModel(150, 100, 50, 0.5)\n",
    "tiny_att_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "tiny_att_doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "tiny_att_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_att_doc_model_hist = tiny_att_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_att_doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.doc_encoder = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001))\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH * MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH * MAX_SENT_LENGTH))\n",
    "        \n",
    "        x = self.doc_encoder(embedded, mask=sent_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "tiny_doc_model = TinyDocModel(150, 100, 0.5)\n",
    "tiny_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "tiny_doc_model(next(iter(train_dataset))[0])\n",
    "\n",
    "tiny_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_doc_model_hist = tiny_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_doc_model_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
