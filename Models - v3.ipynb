{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '3' '1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2\n",
       "1  Our friend, General Joe Ballard owns The Raven...     2\n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3\n",
       "3  Responding to separate emails from Uzra + Jeff...     1\n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN)\n",
    "test_data = load_pandas(TrainOrTest.TEST)\n",
    "\n",
    "print(train_data['label'].unique())\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27432, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='unk')\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['text']\n",
    "    for sent in sent_tokenize(doc)\n",
    "    for word in word_tokenize(sent)\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "vocab_size, t.word_index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[ 3.02399993e-02  4.46060002e-01  4.31659997e-01 -3.75279993e-01\n",
      "  2.90679991e-01  2.30320007e-01  1.81250006e-01  4.02009994e-01\n",
      "  1.35179996e-01 -1.95620000e-01  3.06389987e-01 -1.32390007e-01\n",
      "  6.78969979e-01  4.22340006e-01  3.26370001e-01 -1.52810007e-01\n",
      "  3.76980007e-01 -2.33030006e-01 -3.38169992e-01  3.05880010e-01\n",
      "  4.49180007e-01 -8.36239994e-01  5.91459990e-01  2.49579996e-01\n",
      "  3.99859995e-01 -5.01720011e-01 -2.35440001e-01 -1.46960005e-01\n",
      " -3.51440012e-01 -5.68520010e-01  8.95399973e-02  8.26120019e-01\n",
      " -2.65859991e-01  3.90300006e-01 -3.68489996e-02  4.82569993e-01\n",
      "  7.16639996e-01  1.10040002e-01 -5.93540013e-01 -3.32159996e-01\n",
      " -2.57360011e-01 -3.45310003e-01 -2.63260007e-02 -2.37470001e-01\n",
      "  1.96559995e-04 -2.74800003e-01  3.85120004e-01 -3.95810008e-01\n",
      "  1.14040002e-01 -2.51740009e-01 -3.24699998e-01  8.96079987e-02\n",
      "  2.49290004e-01  1.51269996e+00 -1.97620004e-01 -2.85089993e+00\n",
      " -5.38330019e-01 -4.71109986e-01  1.78590000e+00  7.81260014e-01\n",
      " -1.29629999e-01  5.60769975e-01  3.21509987e-01  3.55710000e-01\n",
      "  8.45470011e-01  1.49309993e-01  1.14869997e-01  3.06250006e-01\n",
      "  5.47739983e-01 -5.04260004e-01  3.38239998e-01 -6.20429993e-01\n",
      " -1.28690004e-02  6.66659996e-02  6.27309978e-02 -4.45340008e-01\n",
      "  1.55410007e-01  2.18009993e-01 -1.73199999e+00  4.20540005e-01\n",
      "  3.63189995e-01 -7.22580031e-02 -7.48109996e-01  1.98880002e-01\n",
      " -1.44610000e+00 -2.75759995e-01  2.66460001e-01 -5.78379989e-01\n",
      "  5.61510026e-01 -2.87009999e-02 -2.46600002e-01 -4.25000012e-01\n",
      " -5.71539998e-01  3.19389999e-01 -2.20750004e-01  4.65279996e-01\n",
      " -1.66060001e-01 -7.99229980e-01  8.08489978e-01  3.73780012e-01]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word, embeddings_index['unk'])\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2858"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(t.word_index.keys() - embeddings_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = \"\"\"Dear abby,\n",
    "\n",
    "I'm writing to tell you you suck. Help me out of this mess.\n",
    "\n",
    "Bye\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Dear', 'abby', ',']], [['I', \"'m\", 'writing', 'to', 'tell', 'you', 'you', 'suck', '.'], ['Help', 'me', 'out', 'of', 'this', 'mess', '.']], [['Bye']]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, tok=None):\n",
    "    return [\n",
    "        [\n",
    "            [\n",
    "                token[0] if token else t.word_index['unk']\n",
    "                for token in tok.texts_to_sequences(nltk.word_tokenize(sent))\n",
    "            ]\n",
    "            if tok else\n",
    "            nltk.word_tokenize(sent)\n",
    "            for sent in nltk.sent_tokenize(para)\n",
    "        ]\n",
    "        for para in text.splitlines()\n",
    "        if len(para) > 0\n",
    "    ]\n",
    "\n",
    "print(tokenize(example_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[536, 11020, 1]], [[5, 108, 1026, 3, 223, 10, 10, 4456, 1], [156, 42, 49, 7, 19, 3063, 1]], [[5009]]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(example_document, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[430, 1]], [[21, 14, 8, 6, 64, 89, 3, 835, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[40, 365, 1, 507, 1920, 8025, 3383, 2, 8026,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[1851, 517, 1], [11497, 15395, 220, 47, 250,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[3546, 3, 1397, 2668, 38, 11501, 1, 559, 1]]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[521, 38, 828, 11, 8, 1398, 4, 11, 5692, 1],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2   \n",
       "1  Our friend, General Joe Ballard owns The Raven...     2   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3   \n",
       "3  Responding to separate emails from Uzra + Jeff...     1   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [[[430, 1]], [[21, 14, 8, 6, 64, 89, 3, 835, 4...  \n",
       "1  [[[40, 365, 1, 507, 1920, 8025, 3383, 2, 8026,...  \n",
       "2  [[[1851, 517, 1], [11497, 15395, 220, 47, 250,...  \n",
       "3  [[[3546, 3, 1397, 2668, 38, 11501, 1, 559, 1]]...  \n",
       "4  [[[521, 38, 828, 11, 8, 1398, 4, 11, 5692, 1],...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tokenized'] = train_data['text'].map(lambda text: tokenize(text, t))\n",
    "test_data['tokenized'] = test_data['text'].map(lambda text: tokenize(text, t))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 255)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DOC_LENGTH = 0\n",
    "MAX_PARA_LENGTH = 0\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for doc in train_data['tokenized'].append(test_data['tokenized']):\n",
    "    MAX_DOC_LENGTH = max(MAX_DOC_LENGTH, len(doc))\n",
    "    for para in doc:\n",
    "        MAX_PARA_LENGTH = max(MAX_PARA_LENGTH, len(para))\n",
    "        for sent in para:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(sent))\n",
    "            \n",
    "MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[430, 1]], [[21, 14, 8, 6, 64, 89, 3, 835, 4480, 3, 441, 7027, 2504, 57, 83, 15, 15389, 8, 799, 1]], [[28, 34, 1, 5, 33, 360, 14, 229, 75, 3, 404, 15, 799, 2071, 4, 5, 25, 48, 2, 2579, 302, 3, 270, 2, 4480, 1], [67, 25, 20, 2505, 3, 69, 12, 2138, 210, 1, 72, 25, 4204, 75, 3, 879, 2017, 12, 3222, 1, 1760, 1, 15390, 1, 294, 1, 4, 19, 2138, 120, 812, 25, 1761, 75, 167, 2, 205, 7, 2, 469, 15, 861, 396, 1, 3544, 1], [19, 399, 33, 157, 42, 250, 210, 41, 34, 7, 5191, 126, 6, 5192, 2418, 12, 6305, 15, 880, 1118, 1]], [[5, 85, 22, 2773, 15, 8022, 49, 3, 6305, 1, 2, 1850, 14, 48, 2, 66, 36, 11, 3, 20, 215, 1, 4, 14, 104, 22, 48, 1716, 1, 1918, 49, 59, 126, 14, 3088, 1, 22, 6, 15391, 15392, 1550, 1, 151, 14, 43, 341, 1802, 9, 2332, 12, 985, 41, 861, 1, 1], [27, 1, 28, 2, 2862, 21, 8, 89, 1, 3956, 4, 5, 33, 58, 3, 79, 49, 986, 8, 182, 3, 1919, 8, 11496, 676, 15, 880, 1803, 1]], [[31, 8023, 2665, 11, 968, 12, 880, 515, 29, 8024, 15, 2, 5688, 677, 1], [638, 281, 14, 5689, 1, 9, 397, 11, 341, 4, 3956, 4, 5, 21, 2666, 12, 2, 120, 1]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 12, 32, 255)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, doc_length=MAX_DOC_LENGTH, para_length=MAX_PARA_LENGTH, sent_length=MAX_SENT_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_length, para_length, sent_length))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                sentnp = np.hstack(np.array(sent))\n",
    "                Z[docidx, paraidx, sentidx, :len(sentnp)] += sentnp\n",
    "    return Z\n",
    "\n",
    "print(train_data['tokenized'][0])\n",
    "padded_example = pad_to_dense(train_data['tokenized'][:1])\n",
    "padded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[430, 1]], [[21, 14, 8, 6, 64, 89, 3, 835, 4480, 3, 441, 7027, 2504, 57, 83, 15, 15389, 8, 799, 1]], [[28, 34, 1, 5, 33, 360, 14, 229, 75, 3, 404, 15, 799, 2071, 4, 5, 25, 48, 2, 2579, 302, 3, 270, 2, 4480, 1], [67, 25, 20, 2505, 3, 69, 12, 2138, 210, 1, 72, 25, 4204, 75, 3, 879, 2017, 12, 3222, 1, 1760, 1, 15390, 1, 294, 1, 4, 19, 2138, 120, 812, 25, 1761, 75, 167, 2, 205, 7, 2, 469, 15, 861, 396, 1, 3544, 1], [19, 399, 33, 157, 42, 250, 210, 41, 34, 7, 5191, 126, 6, 5192, 2418, 12, 6305, 15, 880, 1118, 1]], [[5, 85, 22, 2773, 15, 8022, 49, 3, 6305, 1, 2, 1850, 14, 48, 2, 66, 36, 11, 3, 20, 215, 1, 4, 14, 104, 22, 48, 1716, 1, 1918, 49, 59, 126, 14, 3088, 1, 22, 6, 15391, 15392, 1550, 1, 151, 14, 43, 341, 1802, 9, 2332, 12, 985, 41, 861, 1, 1], [27, 1, 28, 2, 2862, 21, 8, 89, 1, 3956, 4, 5, 33, 58, 3, 79, 49, 986, 8, 182, 3, 1919, 8, 11496, 676, 15, 880, 1803, 1]], [[31, 8023, 2665, 11, 968, 12, 880, 515, 29, 8024, 15, 2, 5688, 677, 1], [638, 281, 14, 5689, 1, 9, 397, 11, 341, 4, 3956, 4, 5, 21, 2666, 12, 2, 120, 1]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 12, 32, 255)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_for(M, doc_length=MAX_DOC_LENGTH, para_length=MAX_PARA_LENGTH, sent_length=MAX_SENT_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_length, para_length, sent_length), dtype=bool)\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                for tokenidx, token in enumerate(sent):\n",
    "                    Z[docidx, paraidx, sentidx, tokenidx] = True\n",
    "    return Z\n",
    "\n",
    "print(train_data['tokenized'][0])\n",
    "masked_example = mask_for(train_data['tokenized'][:1])\n",
    "masked_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 12, 32, 255, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    mask_zero=True)\n",
    "\n",
    "embedded_example = embedding(padded_example)\n",
    "\n",
    "embedded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.cached_attention_weights = []\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # (batch_size, ...) -> (batch_size, 1, ...)\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "class SequenceEncoder(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, recurrent_dropout=0.001)\n",
    "        \n",
    "    def call(self, inputs, mask, training=False):\n",
    "        return self.lstm(\n",
    "            inputs=inputs,\n",
    "            mask=mask,\n",
    "            training=training)\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, attention_units, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_state=True)\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs, mask, training=False):\n",
    "        sequence_encoded, state_h, state_c = self.lstm(\n",
    "            inputs=inputs,\n",
    "            mask=mask,\n",
    "            training=training)\n",
    "        output, attention_weights = self.attention(sequence_encoded, state_h)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "class DocumentEmbeddingClassifier(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden =  tf.keras.layers.Dense(hidden_units)\n",
    "        self.predictor = tf.keras.layers.Dense(3)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.hidden(inputs)\n",
    "        x = self.predictor(x)\n",
    "        \n",
    "        if training:\n",
    "            return x\n",
    "        else:\n",
    "            return tf.math.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorShape([2, 12, 32, 255]),\n",
       " TensorShape([2, 12, 32, 255]),\n",
       " TensorShape([2])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_tensor = pad_to_dense(train_data['tokenized'])\n",
    "test_tensor = pad_to_dense(test_data['tokenized'])\n",
    "train_masks = mask_for(train_data['tokenized'])\n",
    "test_masks = mask_for(test_data['tokenized'])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_tensor, train_masks, train_data['label'].to_numpy()))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_tensor, test_masks, test_data['label'].to_numpy()))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "[item.shape for item in next(iter(train_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in converted code:\n\n    <ipython-input-17-01d2d447258e>:9 mask_reduce  *\n        return any(mask_arr)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n        return py_builtins.overload_of(f)(*args)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:547 __iter__\n        self._disallow_iteration()\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled\n        \" decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-01d2d447258e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mpredict_doc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-01d2d447258e>\u001b[0m in \u001b[0;36mpredict_doc_model\u001b[0;34m(docs, masks, embedder, sent_encoder, para_encoder, doc_encoder, classifier, training)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msent_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msent_mask_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     paras = array_ops.reshape(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mmap_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mback_prop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         maximum_iterations=n)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mresults_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2703\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2704\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2705\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2706\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/map_fn.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(i, tas)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \"\"\"\n\u001b[1;32m    256\u001b[0m       \u001b[0mpacked_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mpacked_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mflat_fn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_fn_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in converted code:\n\n    <ipython-input-17-01d2d447258e>:9 mask_reduce  *\n        return any(mask_arr)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n        return py_builtins.overload_of(f)(*args)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:547 __iter__\n        self._disallow_iteration()\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:540 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:518 _disallow_when_autograph_enabled\n        \" decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n"
     ]
    }
   ],
   "source": [
    "sent_encoder = SequenceEncoder(100)\n",
    "para_encoder = SequenceEncoder(100)\n",
    "doc_encoder = SequenceEncoder(100)\n",
    "classifier = DocumentEmbeddingClassifier(100)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def mask_reduce(mask_arr):\n",
    "    return any(mask_arr)\n",
    "\n",
    "def predict_doc_model(\n",
    "        docs, masks,\n",
    "        embedder, sent_encoder, para_encoder, doc_encoder, classifier,\n",
    "        training=False):\n",
    "    \n",
    "    embedded = embedder(docs)\n",
    "    \n",
    "    sents = array_ops.reshape(\n",
    "        embedded,\n",
    "        (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "    sent_masks = array_ops.reshape(\n",
    "        masks,\n",
    "        (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "    sent_encoded = sent_encoder(sents, sent_masks)\n",
    "    sent_mask_reduced = tf.map_fn(mask_reduce, sent_masks)\n",
    "\n",
    "    paras = array_ops.reshape(\n",
    "        sent_encoded,\n",
    "        (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH, -1))\n",
    "    para_masks = array_ops.reshape(\n",
    "        sent_mask_reduced,\n",
    "        (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH))\n",
    "            \n",
    "    para_encoded = para_encoder(paras, para_masks)\n",
    "    para_mask_reduced = tf.map_fn(mask_reduce, para_masks)\n",
    "    \n",
    "    docs = array_ops.reshape(\n",
    "        para_encoded,\n",
    "        (BATCH_SIZE, MAX_DOC_LENGTH, -1))\n",
    "    docs_masks = array_ops.reshape(\n",
    "        para_mask_reduced,\n",
    "        (BATCH_SIZE, MAX_DOC_LENGTH))\n",
    "    doc_encoded = doc_encoder(docs, docs_masks)\n",
    "    \n",
    "    return classifier(doc_encoded, training=training)\n",
    "\n",
    "\n",
    "predict_doc_model(train_tensor[:2], train_masks[:2], embedding, sent_encoder, para_encoder, doc_encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn_for(predict_fn, embedder, sent_encoder, para_encoder, doc_encoder, classifier):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(docs, masks, targets):\n",
    "        loss = 0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = predict_fn(docs, masks, embedder, sent_encoder, para_encoder, doc_encoder, classifier)\n",
    "            \n",
    "            loss += loss_object(targets, predictions)\n",
    "\n",
    "        variables = sent_encoder.trainable_variables +\\\n",
    "                        para_encoder.trainable_variables +\\\n",
    "                        doc_encoder.trainable_variables +\\\n",
    "                        classifier.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    return train_step\n",
    "    \n",
    "doc_model_train_step = train_fn_for(\n",
    "    predict_doc_model,\n",
    "    embedding,\n",
    "    sent_encoder, para_encoder, doc_encoder,\n",
    "    classifier)\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, mask, target)) in enumerate(train_dataset):\n",
    "        total_loss += doc_model_train_step(inp, mask, target)\n",
    "        \n",
    "        #if batch % 2 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     total_loss.numpy()))\n",
    "        \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
