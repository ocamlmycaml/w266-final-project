{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...    1.0\n",
       "1  Our friend, General Joe Ballard owns The Raven...    1.0\n",
       "2  Outstanding news! Miki Rakic called about 10 m...    2.0\n",
       "3  Responding to separate emails from Uzra + Jeff...    0.0\n",
       "4  Guy from Mexico is in NY and is cooperating. D...    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN)\n",
    "test_data = load_pandas(TrainOrTest.TEST)\n",
    "\n",
    "train_data['label'] = pd.to_numeric(train_data['label']) - 1.0\n",
    "test_data['label'] = pd.to_numeric(test_data['label']) - 1.0\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30660"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['text']\n",
    "    for sent in sent_tokenize(doc)\n",
    "    for word in word_tokenize(sent)\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[-0.38468999  0.99338001  0.13398001 -0.32708001 -0.07744     0.17769\n",
      " -0.071985    0.16159999 -0.13770001  0.051739    0.15964     0.016507\n",
      " -0.049616   -0.53964001  0.24449    -0.62066001 -0.345      -0.015009\n",
      "  0.059399    0.79347998  1.20959997 -0.094457    0.14585    -0.063804\n",
      "  0.1468     -0.50725001 -0.15582    -0.69462001 -0.18542001 -0.20292\n",
      "  0.011547    0.39695001 -0.45813    -0.19921     0.32108     0.54069\n",
      " -0.0073385   0.12096    -0.77902001  0.42853999 -0.53546    -0.58143002\n",
      "  0.14424001 -0.47396001 -0.20623    -0.20815     0.54938    -0.51740998\n",
      "  0.09016    -0.75700998 -0.063903   -0.73684001 -0.097376    1.26349998\n",
      " -0.51025999 -2.56859994  0.47679999 -0.54214001  2.11439991  0.49177\n",
      "  0.21145999  1.57869995 -0.28595999  0.051544    0.4962     -0.26324001\n",
      "  0.82709002  0.50339001  0.90994    -0.28112999  0.020357   -0.63305998\n",
      " -0.33048999 -0.17051999  0.66251999 -0.055619   -0.37652999 -0.15417001\n",
      " -1.16480005 -0.15672     1.29089999  0.12161    -0.48802     0.47591999\n",
      " -1.57410002  0.17321999  0.62256998 -0.041003   -0.52626997 -0.55418998\n",
      "  0.55318999  0.4197     -0.41376999 -0.55962002 -0.81527001 -0.27972001\n",
      " -0.85804999 -0.32359001  0.62028003  0.46397001]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Dear', 'abby', ',']], [['I', \"'m\", 'writing', 'to', 'tell', 'you', 'you', 'suck', '.'], ['Help', 'me', 'out', 'of', 'this', 'mess', '.']], [['Bye']]]\n"
     ]
    }
   ],
   "source": [
    "example_document = \"\"\"Dear abby,\n",
    "\n",
    "I'm writing to tell you you suck. Help me out of this mess.\n",
    "\n",
    "Bye\"\"\"\n",
    "\n",
    "def tokenize(text, tok=t):\n",
    "    return [\n",
    "        [\n",
    "            [\n",
    "                token[0] if token else 0\n",
    "                for token in tok.texts_to_sequences(\n",
    "                    nltk.word_tokenize(sent))\n",
    "            ]\n",
    "            if tok else\n",
    "            nltk.word_tokenize(sent)\n",
    "            for sent in nltk.sent_tokenize(para)\n",
    "        ]\n",
    "        for para in text.splitlines()\n",
    "        if len(para) > 0\n",
    "    ]\n",
    "\n",
    "print(tokenize(example_document, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[533, 11102, 3]], [[6, 115, 1025, 4, 229, 11, 11, 4406, 1], [165, 46, 55, 8, 20, 3016, 1]], [[6155]]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenize(example_document)\n",
    "print(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[[447, 89]], [[22, 15, 9, 7, 71, 98, 4, 821, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[[44, 371, 3, 507, 1927, 8005, 3337, 2, 8006,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[[1817, 529, 35], [11590, 15718, 228, 51, 381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[[3497, 4, 1365, 2722, 42, 11592, 2532, 569, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[[516, 42, 823, 12, 9, 1575, 5, 12, 5628, 1],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...    1.0   \n",
       "1  Our friend, General Joe Ballard owns The Raven...    1.0   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...    2.0   \n",
       "3  Responding to separate emails from Uzra + Jeff...    0.0   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...    0.0   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [[[447, 89]], [[22, 15, 9, 7, 71, 98, 4, 821, ...  \n",
       "1  [[[44, 371, 3, 507, 1927, 8005, 3337, 2, 8006,...  \n",
       "2  [[[1817, 529, 35], [11590, 15718, 228, 51, 381...  \n",
       "3  [[[3497, 4, 1365, 2722, 42, 11592, 2532, 569, ...  \n",
       "4  [[[516, 42, 823, 12, 9, 1575, 5, 12, 5628, 1],...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tokenized'] = train_data['text'].map(lambda text: tokenize(text, t))\n",
    "test_data['tokenized'] = test_data['text'].map(lambda text: tokenize(text, t))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 255)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DOC_LENGTH = 0\n",
    "MAX_PARA_LENGTH = 0\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for doc in train_data['tokenized'].append(test_data['tokenized']):\n",
    "    MAX_DOC_LENGTH = max(MAX_DOC_LENGTH, len(doc))\n",
    "    for para in doc:\n",
    "        MAX_PARA_LENGTH = max(MAX_PARA_LENGTH, len(para))\n",
    "        for sent in para:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(sent))\n",
    "            \n",
    "MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 32, 255)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                Z[docidx, paraidx, sentidx, :len(sent)] += sent\n",
    "    return Z\n",
    "\n",
    "padded_example = pad_to_dense([tokenized_example])\n",
    "padded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4000, 12, 32, 255]), TensorShape([800, 12, 32, 255]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = tf.convert_to_tensor(pad_to_dense(train_data['tokenized']))\n",
    "test_tensor = tf.convert_to_tensor(pad_to_dense(test_data['tokenized']))\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4000, 5]), TensorShape([800, 5]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorical_labels(labels):\n",
    "    eye = [\n",
    "        [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for item in labels:\n",
    "        result.append(eye[int(item)])\n",
    "        \n",
    "    return result\n",
    "\n",
    "train_labels = tf.convert_to_tensor(categorical_labels(train_data['label']))\n",
    "test_labels = tf.convert_to_tensor(categorical_labels(test_data['label']))\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 12, 32, 255), TensorShape([1, 12, 32, 255, 100]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    mask_zero=True)\n",
    "\n",
    "embedded_example = []\n",
    "for doc in padded_example:\n",
    "    embedded_example.append([])\n",
    "    for para in doc:\n",
    "        embedded_example[-1].append([])\n",
    "        for sent in para:\n",
    "            embedded_example[-1][-1].append(embedding(sent))\n",
    "\n",
    "embedded_example = tf.convert_to_tensor(embedded_example)\n",
    "padded_example.shape, embedded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.cached_attention_weights = []\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # (batch_size, ...) -> (batch_size, 1, ...)\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_state=True)\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sequence_encoded, state_h, state_c = self.lstm(inputs)\n",
    "        output, attention_weights = self.attention(sequence_encoded, state_h)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.embedder = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            mask_zero=True)\n",
    "        self.sent_encoder = tf.keras.layers.LSTM(lstm_units)\n",
    "        self.para_encoder = tf.keras.layers.LSTM(lstm_units)\n",
    "        self.doc_encoder = tf.keras.layers.LSTM(lstm_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.classifier = tf.keras.layers.Dense(5, activation='sigmoid')\n",
    "\n",
    "def train_fn_for()\n",
    "    @tf.function\n",
    "    def train_fn(docs, targ):\n",
    "        emb_docs = []\n",
    "        for doc in docs:\n",
    "            emb_paras = []\n",
    "            for para in doc:\n",
    "                emb_sents = []\n",
    "                for sent in para:\n",
    "                    emb_sents.append(self.embedder(sent))\n",
    "                enc_sents = self.sent_encoder(tf.convert_to_tensor(emb_sents))\n",
    "                emb_paras.append(enc_sents)\n",
    "            enc_paras = self.para_encoder(tf.convert_to_tensor(emb_paras))\n",
    "            emb_docs.append(enc_paras)\n",
    "        \n",
    "        enc_docs = self.doc_encoder(tf.convert_to_tensor(emb_docs))\n",
    "        hidden_pass = self.hidden(enc_docs)\n",
    "        return self.classifier(hidden_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
