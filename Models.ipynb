{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Cheryl, :, Are, we, in, a, good, place, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Our, friend, ,, General, Joe, Ballard, owns,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Outstanding, news, !], [Miki, Rakic, called,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Responding, to, separate, emails, from, Uzra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Guy, from, Mexico, is, in, NY, and, is, coop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2   \n",
       "1  Our friend, General Joe Ballard owns The Raven...     2   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3   \n",
       "3  Responding to separate emails from Uzra + Jeff...     1   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1   \n",
       "\n",
       "                                               sents  \n",
       "0  [[Cheryl, :, Are, we, in, a, good, place, to, ...  \n",
       "1  [[Our, friend, ,, General, Joe, Ballard, owns,...  \n",
       "2  [[Outstanding, news, !], [Miki, Rakic, called,...  \n",
       "3  [[Responding, to, separate, emails, from, Uzra...  \n",
       "4  [[Guy, from, Mexico, is, in, NY, and, is, coop...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN)\n",
    "test_data = load_pandas(TrainOrTest.TEST)\n",
    "\n",
    "train_data['sents'] = train_data['text'].map(lambda t: [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(t)\n",
    "])\n",
    "test_data['sents'] = test_data['text'].map(lambda t: [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(t)\n",
    "])\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['sents']\n",
    "    for sent in doc\n",
    "    for word in sent\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "def encode(texts):\n",
    "    return [\n",
    "        t.texts_to_sequences(doc) for doc in texts\n",
    "    ]\n",
    "\n",
    "encoded_train = encode(train_data['sents'])\n",
    "encoded_test = encode(test_data['sents'])\n",
    "\n",
    "max_sent_length = max(\n",
    "    max(len(sent) for txt in encoded_train for sent in txt),\n",
    "    max(len(sent) for txt in encoded_test for sent in txt)\n",
    ")\n",
    "max_doc_length = max(\n",
    "    max(len(txt) for txt in encoded_train),\n",
    "    max(len(txt) for txt in encoded_test)\n",
    ")\n",
    "\n",
    "max_sent_length, max_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 32, 239), (800, 32, 239))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, sent_len, doc_len):\n",
    "    maxlen = max(len(r) for r in M)\n",
    "\n",
    "    Z = np.zeros((len(M), doc_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for sentidx, sent in enumerate(doc):\n",
    "            Z[docidx, sentidx, :len(sent)] += np.array(sent)\n",
    "    return Z\n",
    "\n",
    "train_tensor = pad_to_dense(encoded_train, max_sent_length, max_doc_length)\n",
    "test_tensor = pad_to_dense(encoded_test, max_sent_length, max_doc_length)\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem, why does it matter - metric\n",
    "\n",
    "# previous work\n",
    "\n",
    "# my approach\n",
    "\n",
    "# outcome - good and bad\n",
    "\n",
    "# what would I do if i have more time\n",
    "\n",
    "audience is instructors for report, but other students for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.cached_attention_weights = []\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # (batch_size, ...) -> (batch_size, 1, ...)\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_state=True)\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        encoded, state_h, state_c = self.lstm(inputs)\n",
    "        output, attention_weights = self.attention(sequence_encoded, state_h)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(vocab_size, 32, input_shape=(max_doc_length, max_sent_length))\n",
    "sent_encoder = tf.keras.layers.TimeDistributed(\n",
    "    tf.keras.layers.TimeDistributed(\n",
    "        AttentiveSequenceEncoder(64, 256)))\n",
    "\n",
    "doc_encoder = tf.keras.layers.TimeDistributed(AttentiveSequenceEncoder(64, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  535.     0.     0. ...     0.     0.     0.]\n",
      "  [11020.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  ...\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]]\n",
      "\n",
      " [[ 5008.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  ...\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 32, 239)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_document = [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(\"\\n\".join([\n",
    "        \"Dear abby,\"\n",
    "        \"I'm writing to tell you you suck.\"\n",
    "        \"Help me out of this mess.\",\n",
    "        \"Bye\"\n",
    "    ]))\n",
    "]\n",
    "\n",
    "encoded_example = pad_to_dense(encode(example_document), max_sent_length, max_doc_length)\n",
    "print(encoded_example)\n",
    "encoded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(32), Dimension(239), Dimension(32)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(encoded_example).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [64, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-7630730c4141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0minner_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m       \u001b[0;31m# Shape: (num_samples, timesteps, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0minitial_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           unroll=False)\n\u001b[0m\u001b[1;32m    236\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   3730\u001b[0m     \u001b[0;31m# the value is discarded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3731\u001b[0m     output_time_zero, _ = step_function(\n\u001b[0;32m-> 3732\u001b[0;31m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[1;32m   3733\u001b[0m     output_ta = tuple(\n\u001b[1;32m   3734\u001b[0m         tensor_array_ops.TensorArray(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(x, _)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0;31m# batch size matters, use rnn-based implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-2a620a18d39c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# the corresponding TF subgraph inside `backend.get_graph()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 586\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [64, 32]"
     ]
    }
   ],
   "source": [
    "sent_encoder(embedding(encoded_example)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
