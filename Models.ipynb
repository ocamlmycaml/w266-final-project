{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sents</th>\n",
       "      <th>label_normed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Cheryl, :, Are, we, in, a, good, place, to, ...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Our, friend, ,, General, Joe, Ballard, owns,...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Outstanding, news, !], [Miki, Rakic, called,...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Responding, to, separate, emails, from, Uzra...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Guy, from, Mexico, is, in, NY, and, is, coop...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2   \n",
       "1  Our friend, General Joe Ballard owns The Raven...     2   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3   \n",
       "3  Responding to separate emails from Uzra + Jeff...     1   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1   \n",
       "\n",
       "                                               sents  label_normed  \n",
       "0  [[Cheryl, :, Are, we, in, a, good, place, to, ...           0.4  \n",
       "1  [[Our, friend, ,, General, Joe, Ballard, owns,...           0.4  \n",
       "2  [[Outstanding, news, !], [Miki, Rakic, called,...           0.6  \n",
       "3  [[Responding, to, separate, emails, from, Uzra...           0.2  \n",
       "4  [[Guy, from, Mexico, is, in, NY, and, is, coop...           0.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN)\n",
    "test_data = load_pandas(TrainOrTest.TEST)\n",
    "\n",
    "train_data['sents'] = train_data['text'].map(lambda t: [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(t)\n",
    "])\n",
    "test_data['sents'] = test_data['text'].map(lambda t: [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(t)\n",
    "])\n",
    "\n",
    "train_data['label_normed'] = pd.to_numeric(train_data['label']) / 5.0\n",
    "test_data['label_normed'] = pd.to_numeric(test_data['label']) / 5.0\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27431, 239, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['sents']\n",
    "    for sent in doc\n",
    "    for word in sent\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "def encode(texts):\n",
    "    return [\n",
    "        t.texts_to_sequences(doc) for doc in texts\n",
    "    ]\n",
    "\n",
    "encoded_train = encode(train_data['sents'])\n",
    "encoded_test = encode(test_data['sents'])\n",
    "\n",
    "max_sent_length = max(\n",
    "    max(len(sent) for txt in encoded_train for sent in txt),\n",
    "    max(len(sent) for txt in encoded_test for sent in txt)\n",
    ")\n",
    "max_doc_length = max(\n",
    "    max(len(txt) for txt in encoded_train),\n",
    "    max(len(txt) for txt in encoded_test)\n",
    ")\n",
    "\n",
    "vocab_size, max_sent_length, max_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 32, 239), (800, 32, 239))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, sent_len, doc_len):\n",
    "    maxlen = max(len(r) for r in M)\n",
    "\n",
    "    Z = np.zeros((len(M), doc_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for sentidx, sent in enumerate(doc):\n",
    "            Z[docidx, sentidx, :len(sent)] += np.array(sent)\n",
    "    return Z\n",
    "\n",
    "train_tensor = pad_to_dense(encoded_train, max_sent_length, max_doc_length)\n",
    "test_tensor = pad_to_dense(encoded_test, max_sent_length, max_doc_length)\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem, why does it matter - metric\n",
    "\n",
    "# previous work\n",
    "\n",
    "# my approach\n",
    "\n",
    "# outcome - good and bad\n",
    "\n",
    "# what would I do if i have more time\n",
    "\n",
    "audience is instructors for report, but other students for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 32, 239, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2  # keep it small for GPU ram porpoises\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False,\n",
    "                            input_shape=(max_doc_length, max_sent_length),\n",
    "                           mask_zero=True)\n",
    "\n",
    "embedding(train_tensor[:BATCH_SIZE]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  535.     0.     0. ...     0.     0.     0.]\n",
      "  [11019.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  ...\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]]\n",
      "\n",
      " [[ 5008.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  ...\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]\n",
      "  [    0.     0.     0. ...     0.     0.     0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 32, 239)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_document = [\n",
    "    word_tokenize(sent)\n",
    "    for sent in sent_tokenize(\"\\n\".join([\n",
    "        \"Dear abby,\"\n",
    "        \"I'm writing to tell you you suck.\"\n",
    "        \"Help me out of this mess.\",\n",
    "        \"Bye\"\n",
    "    ]))\n",
    "]\n",
    "\n",
    "encoded_example = pad_to_dense(encode(example_document), max_sent_length, max_doc_length)\n",
    "print(encoded_example)\n",
    "encoded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 32, 239, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_example = embedding(encoded_example)\n",
    "embedded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 239, 100)\n",
      "(64, 239, 100)\n",
      "(64, 1024)\n",
      "(2, 32, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_flatten = lambda t: tf.reshape(t, (BATCH_SIZE * max_doc_length, max_sent_length, -1))\n",
    "test_lstm = tf.keras.layers.LSTM(1024)\n",
    "test_reshape = lambda t: tf.reshape(t, (BATCH_SIZE, max_doc_length, 1024))\n",
    "\n",
    "print(embedded_example.shape)\n",
    "print(test_flatten(embedded_example).shape)\n",
    "print(test_lstm(test_flatten(embedded_example)).shape)\n",
    "print(test_reshape(test_lstm(test_flatten(embedded_example))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.cached_attention_weights = []\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # (batch_size, ...) -> (batch_size, 1, ...)\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_state=True)\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sequence_encoded, state_h, state_c = self.lstm(inputs)\n",
    "        output, attention_weights = self.attention(sequence_encoded, state_h)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32, 239, 100)\n",
      "(64, 239, 100)\n",
      "(64, 1024)\n",
      "(2, 32, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_flatten = lambda t: tf.reshape(t, (2 * max_doc_length, max_sent_length, -1))\n",
    "test_lstm = AttentiveSequenceEncoder(1024, 128)\n",
    "test_reshape = lambda t: tf.reshape(t, (2, max_doc_length, 1024))\n",
    "\n",
    "print(embedded_example.shape)\n",
    "print(test_flatten(embedded_example).shape)\n",
    "print(test_lstm(test_flatten(embedded_example))[0].shape)\n",
    "print(test_reshape(test_lstm(test_flatten(embedded_example))[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_matrix, lstm_units,\n",
    "                 batch_size, max_doc_length, max_sent_length):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            len(embedding_matrix[0]),\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(max_doc_length, max_sent_length),\n",
    "            mask_zero=True)\n",
    "        self.sentence_encoder = tf.keras.layers.LSTM(lstm_units)\n",
    "        self.document_encoder = tf.keras.layers.LSTM(lstm_units)\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation=tf.nn.tanh)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_doc_length = max_doc_length\n",
    "        self.max_sent_length = max_sent_length\n",
    "    \n",
    "    def flatten_docs(self, t):\n",
    "        return tf.reshape(t, (self.batch_size * self.max_doc_length, self.max_sent_length, -1))\n",
    "    \n",
    "    def inflate_docs(self, t):\n",
    "        return tf.reshape(t, (self.batch_size, self.max_doc_length, -1))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.flatten_docs(x)\n",
    "        x = self.sentence_encoder(x)\n",
    "        x = self.inflate_docs(x)\n",
    "        x = self.document_encoder(x)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveDocumentClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_matrix, lstm_units, attention_units,\n",
    "                 batch_size, max_doc_length, max_sent_length):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            len(embedding_matrix[0]),\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(max_doc_length, max_sent_length),\n",
    "            mask_zero=True)\n",
    "        self.sentence_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.document_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, activation=tf.nn.tanh)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_doc_length = max_doc_length\n",
    "        self.max_sent_length = max_sent_length\n",
    "    \n",
    "    def flatten_docs(self, t):\n",
    "        return tf.reshape(t, (self.batch_size * self.max_doc_length, self.max_sent_length, -1))\n",
    "    \n",
    "    def inflate_docs(self, t):\n",
    "        return tf.reshape(t, (self.batch_size, self.max_doc_length, -1))\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.flatten_docs(x)\n",
    "        x, _ = self.sentence_encoder(x)\n",
    "        x = self.inflate_docs(x)\n",
    "        x, _ = self.document_encoder(x)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer document_classifier_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=35282, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [1.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_model = DocumentClassifierModel(vocab_size, embedding_matrix, 1024,\n",
    "                                    BATCH_SIZE, max_doc_length, max_sent_length)\n",
    "attentive_doc_model = AttentiveDocumentClassifierModel(vocab_size, embedding_matrix, 1024, 32,\n",
    "                                                       BATCH_SIZE, max_doc_length, max_sent_length)\n",
    "\n",
    "doc_model(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer attentive_document_classifier_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=43456, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [1.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentive_doc_model(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.BinaryCrossentropy())\n",
    "attentive_doc_model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples\n",
      "Epoch 1/10\n",
      "  72/4000 [..............................] - ETA: 4:04:00 - loss: 8.9870 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "doc_model.fit(\n",
    "    train_tensor, train_data['label_normed'].to_numpy(),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
