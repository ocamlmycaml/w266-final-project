{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.python.ops import array_ops\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, Source, TrainOrTest\n",
    "\n",
    "def pythonize(history):\n",
    "    return {\n",
    "        key: [float(x) for x in scores]\n",
    "        for key, scores in history.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '3' '1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2\n",
       "1  Our friend, General Joe Ballard owns The Raven...     2\n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3\n",
       "3  Responding to separate emails from Uzra + Jeff...     1\n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN, sources=[Source.CLINTON, Source.ENRON])\n",
    "test_data = load_pandas(TrainOrTest.TEST, sources=[Source.CLINTON, Source.ENRON])\n",
    "\n",
    "print(train_data['label'].unique())\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18832, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='unk')\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['text']\n",
    "    for sent in sent_tokenize(doc)\n",
    "    for word in word_tokenize(sent)\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "vocab_size, t.word_index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[ 1.28830001e-01 -8.22090030e-01  2.74379998e-01 -6.90139979e-02\n",
      "  1.79890007e-01  7.26050019e-01 -1.51120007e-01  8.55410006e-03\n",
      " -9.51219976e-01  7.72430003e-01 -2.83749998e-01  2.83289999e-01\n",
      "  1.48249999e-01 -1.22300005e-02 -1.92670003e-02 -3.44600007e-02\n",
      "  3.15059990e-01 -1.66390002e-01 -1.34349996e-02 -2.04590010e-03\n",
      "  6.49050027e-02 -2.09889993e-01  1.25239998e-01  3.52299988e-01\n",
      "  6.40399992e-01  5.95699996e-02 -8.03020000e-01 -8.16479981e-01\n",
      "  6.61339998e-01  5.99699989e-02 -6.15210012e-02  8.49219978e-01\n",
      " -2.87330002e-02  2.76699990e-01 -1.00680006e+00  7.17580020e-01\n",
      " -3.72570008e-01  4.30640012e-01 -4.92439985e-01  3.86830002e-01\n",
      " -3.68279994e-01  2.79820003e-02  1.53460002e+00 -6.05329990e-01\n",
      " -3.44489992e-01 -1.70690000e-01  2.92879999e-01 -5.35809994e-01\n",
      "  5.60350001e-01 -6.30129993e-01 -1.23080000e-01  9.36330035e-02\n",
      "  5.93360007e-01  1.52139997e+00 -9.26290005e-02 -3.14080000e+00\n",
      "  1.39310002e-01 -5.38200021e-01  1.17359996e+00  6.23179972e-01\n",
      "  4.36210006e-01  1.28559995e+00  1.21210001e-01  4.62060004e-01\n",
      "  5.61420023e-01 -4.14389998e-01  9.43599999e-01  3.89539987e-01\n",
      " -5.31559996e-02  1.86220005e-01 -1.87849998e-01  3.76029998e-01\n",
      "  1.38779998e-01 -1.28810000e+00  1.85340002e-01  3.51570010e-01\n",
      " -8.08879972e-01 -6.76620007e-02 -1.19340003e+00  2.00949997e-01\n",
      "  9.62970018e-01  9.20740008e-01 -3.09330001e-02 -1.17430001e-01\n",
      " -1.52100003e+00 -7.75390029e-01 -9.11780000e-02 -1.27739996e-01\n",
      " -6.39580011e-01 -6.80989981e-01 -1.60370007e-01 -2.17319995e-01\n",
      "  5.70879996e-01  8.66880000e-01 -6.78510010e-01 -6.06410027e-01\n",
      " -6.89270020e-01 -3.39610010e-01  4.27430004e-01  1.65749997e-01]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word, embeddings_index['unk'])\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(t.word_index.keys() - embeddings_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = \"\"\"Dear abby,\n",
    "\n",
    "I'm writing to tell you you suck. Help me out of this mess.\n",
    "\n",
    "Bye\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Dear', 'abby', ',']], [['I', \"'m\", 'writing', 'to', 'tell', 'you', 'you', 'suck', '.'], ['Help', 'me', 'out', 'of', 'this', 'mess', '.']], [['Bye']]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, tok=None):\n",
    "    return [\n",
    "        [\n",
    "            tok.texts_to_sequences(nltk.word_tokenize(sent))\n",
    "            if tok else\n",
    "            nltk.word_tokenize(sent)\n",
    "            for sent in nltk.sent_tokenize(para)\n",
    "        ]\n",
    "        for para in text.splitlines()\n",
    "        if len(para) > 0\n",
    "    ]\n",
    "\n",
    "print(tokenize(example_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[310], [1], []]], [[[6], [115], [997], [3], [379], [12], [12], [1], []], [[140], [37], [51], [5], [16], [5216], []]], [[[4542]]]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(example_document, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[[239], []]], [[[22], [13], [8], [7], [106],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[[30], [1104], [], [399], [1482], [5415], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[[1727], [485], []], [[7862], [10654], [240]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[[2476], [3], [1134], [2020], [28], [7867], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[[1106], [28], [783], [11], [8], [1012], [4]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2   \n",
       "1  Our friend, General Joe Ballard owns The Raven...     2   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3   \n",
       "3  Responding to separate emails from Uzra + Jeff...     1   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [[[[239], []]], [[[22], [13], [8], [7], [106],...  \n",
       "1  [[[[30], [1104], [], [399], [1482], [5415], [2...  \n",
       "2  [[[[1727], [485], []], [[7862], [10654], [240]...  \n",
       "3  [[[[2476], [3], [1134], [2020], [28], [7867], ...  \n",
       "4  [[[[1106], [28], [783], [11], [8], [1012], [4]...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tokenized'] = train_data['text'].map(lambda text: tokenize(text, t))\n",
    "test_data['tokenized'] = test_data['text'].map(lambda text: tokenize(text, t))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 32, 255)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DOC_LENGTH = 0\n",
    "MAX_PARA_LENGTH = 0\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for doc in train_data['tokenized'].append(test_data['tokenized']):\n",
    "    MAX_DOC_LENGTH = max(MAX_DOC_LENGTH, len(doc))\n",
    "    for para in doc:\n",
    "        MAX_PARA_LENGTH = max(MAX_PARA_LENGTH, len(para))\n",
    "        for sent in para:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(sent))\n",
    "            \n",
    "MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[239], []]], [[[22], [13], [8], [7], [106], [225], [3], [606], [3809], [3], [484], [4721], [1653], [57], [118], [14], [10647], [8], [523], []]], [[[27], [43], [], [6], [26], [1054], [13], [318], [67], [3], [248], [14], [523], [1654], [4], [6], [19], [56], [2], [4722], [220], [3], [305], [2], [3809], []], [[69], [19], [17], [1943], [3], [53], [10], [1723], [161], [], [64], [19], [3222], [67], [3], [650], [1317], [10], [2797], [], [4723], [], [10648], [], [298], [], [4], [16], [1723], [121], [667], [19], [2624], [67], [138], [2], [176], [5], [2], [321], [14], [544], [459], [], [3223], []], [[16], [242], [26], [179], [37], [233], [161], [41], [43], [5], [4224], [131], [7], [4225], [1597], [10], [4724], [14], [537], [916], []]], [[[6], [66], [25], [1797], [14], [7860], [51], [3], [4724], [], [2], [2018], [13], [56], [2], [60], [46], [11], [3], [17], [211], [], [4], [13], [74], [25], [56], [2625], [], [4725], [51], [49], [131], [13], [2019], [], [25], [7], [10649], [10650], [1103], [], [151], [13], [42], [273], [1864], [9], [1865], [10], [612], [41], [544], [], []], [[31], [], [27], [2], [2217], [22], [8], [225], [], [3224], [4], [6], [26], [71], [3], [149], [51], [700, 537], [8], [230], [3], [1318], [8], [7861], [417], [14], [537], [1209, 268], []]], [[[47], [5413, 8], [1866], [11], [651], [10], [537], [499], [24], [5414], [14], [2], [3810], [584], []], [[1011], [187], [13], [4726], [], [9], [283], [11], [273], [4], [3224], [4], [6], [22], [1798], [10], [2], [121], []]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[239.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 22.,  13.,   8., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 27.,  43.,   6., ...,   0.,   0.,   0.],\n",
       "         [ 69.,  19.,  17., ...,   0.,   0.,   0.],\n",
       "         [ 16., 242.,  26., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                sentnp = np.hstack(np.array(sent))\n",
    "                Z[docidx, paraidx, sentidx, :len(sentnp)] += sentnp\n",
    "    return Z\n",
    "\n",
    "print(train_data['tokenized'][0])\n",
    "pad_to_dense(train_data['tokenized'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ True,  True, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False],\n",
       "         [False, False, False, ..., False, False, False]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dense_mask(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len), dtype=bool)\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                for tokenidx, token in enumerate(sent):\n",
    "                    Z[docidx, paraidx, sentidx, tokenidx] = True\n",
    "    return Z\n",
    "\n",
    "dense_mask(train_data['tokenized'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 12, 32, 255), (400, 12, 32, 255))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = pad_to_dense(train_data['tokenized'])\n",
    "test_tensor = pad_to_dense(test_data['tokenized'])\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 3), (400, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorical_labels(labels):\n",
    "    eye = [\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for item in labels:\n",
    "        result.append(eye[int(item) - 1])\n",
    "        \n",
    "    return np.array(result)\n",
    "\n",
    "train_labels = categorical_labels(pd.to_numeric(train_data['label']))\n",
    "test_labels = categorical_labels(pd.to_numeric(test_data['label']))\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((2000, 12, 32, 255), (2000, 12, 32), (2000, 12)),\n",
       " ((400, 12, 32, 255), (400, 12, 32), (400, 12)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent_mask = dense_mask(train_data['tokenized'])\n",
    "test_sent_mask = dense_mask(test_data['tokenized'])\n",
    "\n",
    "train_para_mask = np.apply_along_axis(any, 3, train_sent_mask)\n",
    "test_para_mask = np.apply_along_axis(any, 3, test_sent_mask)\n",
    "\n",
    "train_doc_mask = np.apply_along_axis(any, 2, train_para_mask)\n",
    "test_doc_mask = np.apply_along_axis(any, 2, test_para_mask)\n",
    "\n",
    "(train_sent_mask.shape, train_para_mask.shape, train_doc_mask.shape), (test_sent_mask.shape, test_para_mask.shape, test_doc_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 12, 32, 255]),\n",
       " TensorShape([2, 12, 32, 255]),\n",
       " TensorShape([2, 12, 32]),\n",
       " TensorShape([2, 12]),\n",
       " TensorShape([2, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((train_tensor, train_sent_mask, train_para_mask, train_doc_mask), train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ((test_tensor, test_sent_mask, test_para_mask, test_doc_mask), test_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "(a, b, c, d), e = next(iter(train_dataset))\n",
    "a.shape, b.shape, c.shape, d.shape, e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 32, 255, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=73, shape=(100,), dtype=float32, numpy=\n",
       "array([ 0.12616  , -0.2641   ,  0.071929 , -0.96047  , -0.086358 ,\n",
       "       -0.032276 ,  0.33611  ,  0.55126  , -0.19631  , -0.66443  ,\n",
       "       -0.16571  , -0.40401  , -0.13921  ,  0.44772  , -0.039273 ,\n",
       "       -0.63014  , -0.2928   , -0.024373 , -0.81073  ,  0.70829  ,\n",
       "       -0.47717  ,  0.39068  ,  0.44993  ,  0.25711  ,  0.68318  ,\n",
       "        0.14003  , -0.013181 , -1.212    , -0.14414  ,  0.21759  ,\n",
       "        0.30636  ,  0.7272   ,  0.82667  , -0.20531  , -0.68931  ,\n",
       "       -0.047831 ,  0.3048   ,  0.20761  ,  0.33063  ,  0.33195  ,\n",
       "       -0.23914  ,  0.046714 , -0.46688  ,  0.46208  ,  0.29071  ,\n",
       "        0.60412  , -0.75673  , -0.34308  , -0.32161  , -0.17654  ,\n",
       "        0.66982  ,  0.014476 , -0.12332  , -0.29709  ,  0.26196  ,\n",
       "       -0.49916  , -0.65069  ,  0.3813   , -0.76894  , -0.2284   ,\n",
       "       -0.25254  , -0.27246  ,  0.38411  ,  0.52052  ,  0.05651  ,\n",
       "       -0.49209  ,  1.0191   ,  0.20061  , -0.33445  , -0.0094115,\n",
       "        0.1643   ,  0.15405  , -0.57942  , -0.38563  ,  0.21205  ,\n",
       "        0.55645  , -0.047173 , -0.094341 , -0.24525  , -0.59299  ,\n",
       "       -0.5196   , -0.098982 ,  0.32377  ,  0.5476   , -0.022338 ,\n",
       "       -0.43968  ,  0.38285  , -0.24398  , -0.23914  , -0.12387  ,\n",
       "        0.45618  ,  0.46133  ,  0.17065  , -0.29166  ,  0.43681  ,\n",
       "        0.50118  , -0.11174  , -0.2478   , -0.30651  ,  0.44494  ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False)\n",
    "\n",
    "embedded_example = embedding(train_tensor[:2])\n",
    "\n",
    "print(embedded_example.shape)\n",
    "embedded_example[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, values):\n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W(values)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = Bidirectional(LSTM(lstm_units, recurrent_dropout=0.0001, return_sequences=True))\n",
    "        self.concat = Concatenate()\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs, mask):\n",
    "        encoded = self.lstm(inputs, mask=mask)\n",
    "        output, attention_weights = self.attention(encoded)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.para_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded, sent_weights = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH, -1))\n",
    "        para_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH))\n",
    "        \n",
    "        para_embedded, para_weights = self.para_encoder(sent_embedded, mask=para_mask)\n",
    "        para_embedded = array_ops.reshape(\n",
    "            para_embedded, (BATCH_SIZE, MAX_DOC_LENGTH, -1))\n",
    "        \n",
    "        x, doc_weights = self.doc_encoder(para_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if not training:\n",
    "            self.sent_weights = sent_weights\n",
    "            self.para_weights = para_weights\n",
    "            self.doc_weights = doc_weights\n",
    "        \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer attentive_doc_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 788s 788ms/step - loss: 1.0403 - accuracy: 0.5075 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 780s 780ms/step - loss: 1.0339 - accuracy: 0.5110 - val_loss: 1.0254 - val_accuracy: 0.5075\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 779s 779ms/step - loss: 1.0151 - accuracy: 0.5280 - val_loss: 1.0204 - val_accuracy: 0.5075\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 777s 777ms/step - loss: 1.0095 - accuracy: 0.5315 - val_loss: 1.0228 - val_accuracy: 0.5025\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 775s 775ms/step - loss: 1.0021 - accuracy: 0.5430 - val_loss: 1.0436 - val_accuracy: 0.4625\n",
      "WARNING:tensorflow:Layer attentive_doc_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 798s 798ms/step - loss: 1.0316 - accuracy: 0.5105 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 788s 788ms/step - loss: 1.0029 - accuracy: 0.5290 - val_loss: 1.0162 - val_accuracy: 0.5175\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 787s 787ms/step - loss: 0.9661 - accuracy: 0.5555 - val_loss: 1.0151 - val_accuracy: 0.5275\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 790s 790ms/step - loss: 0.9453 - accuracy: 0.5745 - val_loss: 1.0177 - val_accuracy: 0.5025\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 789s 789ms/step - loss: 0.9239 - accuracy: 0.5885 - val_loss: 0.9799 - val_accuracy: 0.5650\n",
      "WARNING:tensorflow:Layer attentive_doc_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 810s 810ms/step - loss: 1.0325 - accuracy: 0.5085 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 802s 802ms/step - loss: 1.0308 - accuracy: 0.5090 - val_loss: 1.0398 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 797s 797ms/step - loss: 1.0268 - accuracy: 0.5090 - val_loss: 1.0377 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 796s 796ms/step - loss: 1.0272 - accuracy: 0.5090 - val_loss: 1.0368 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 820s 820ms/step - loss: 1.0402 - accuracy: 0.5080 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 809s 809ms/step - loss: 1.0254 - accuracy: 0.5115 - val_loss: 1.0368 - val_accuracy: 0.4975\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 806s 806ms/step - loss: 1.0299 - accuracy: 0.5105 - val_loss: 1.0424 - val_accuracy: 0.4975\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 807s 807ms/step - loss: 1.0261 - accuracy: 0.5140 - val_loss: 1.0250 - val_accuracy: 0.5050\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 807s 807ms/step - loss: 1.0203 - accuracy: 0.5110 - val_loss: 1.0167 - val_accuracy: 0.5225\n",
      "Model: \"attentive_doc_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  1883200   \n",
      "_________________________________________________________________\n",
      "attentive_sequence_encoder_6 multiple                  361601    \n",
      "_________________________________________________________________\n",
      "attentive_sequence_encoder_7 multiple                  601601    \n",
      "_________________________________________________________________\n",
      "attentive_sequence_encoder_8 multiple                  601601    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             multiple                  90300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             multiple                  903       \n",
      "=================================================================\n",
      "Total params: 3,539,206\n",
      "Trainable params: 1,656,006\n",
      "Non-trainable params: 1,883,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 0.565)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_attentive_doc_model = None\n",
    "best_units = None\n",
    "best_val_score = 0.0\n",
    "\n",
    "for attention_units in (100, 200, 300, 400):\n",
    "    attentive_doc_model = AttentiveDocModel(150, 300, attention_units, 0.5)\n",
    "    attentive_doc_model.compile(optimizer='adam', metrics=['accuracy'https://manjaro.org/, 'categorical_accuracy'],\n",
    "                                loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "    attentive_doc_model(next(iter(train_dataset))[0])\n",
    "    attentive_doc_model_hist = attentive_doc_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=5)\n",
    "    \n",
    "    with open(f'attentive_doc_model_history_{attention_units}.json', 'w') as f:\n",
    "        json.dump(pythonize(attentive_doc_model_hist.history), f)\n",
    "        \n",
    "    val_accuracy = max(attentive_doc_model_hist.history['val_accuracy'])\n",
    "    if val_accuracy > best_val_score:\n",
    "        best_attentive_doc_model = attentive_doc_model\n",
    "        best_units = attention_units\n",
    "        best_val_score = val_accuracy\n",
    "\n",
    "best_attentive_doc_model.summary()\n",
    "best_units, best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer attentive_doc_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 881s 881ms/step - loss: 1.0334 - accuracy: 0.5040 - categorical_accuracy: 0.5040 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 862s 862ms/step - loss: 1.0254 - accuracy: 0.5080 - categorical_accuracy: 0.5080 - val_loss: 1.0281 - val_accuracy: 0.4975 - val_categorical_accuracy: 0.4975\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 863s 863ms/step - loss: 1.0270 - accuracy: 0.5140 - categorical_accuracy: 0.5140 - val_loss: 1.0303 - val_accuracy: 0.4975 - val_categorical_accuracy: 0.4975\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 861s 861ms/step - loss: 1.0183 - accuracy: 0.5195 - categorical_accuracy: 0.5195 - val_loss: 1.0332 - val_accuracy: 0.5050 - val_categorical_accuracy: 0.5050\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 863s 863ms/step - loss: 1.0137 - accuracy: 0.5235 - categorical_accuracy: 0.5235 - val_loss: 1.0181 - val_accuracy: 0.5075 - val_categorical_accuracy: 0.5075\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 867s 867ms/step - loss: 1.0066 - accuracy: 0.5325 - categorical_accuracy: 0.5325 - val_loss: 1.0238 - val_accuracy: 0.5100 - val_categorical_accuracy: 0.5100\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 862s 862ms/step - loss: 1.0014 - accuracy: 0.5380 - categorical_accuracy: 0.5380 - val_loss: 1.0250 - val_accuracy: 0.5150 - val_categorical_accuracy: 0.5150\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 863s 863ms/step - loss: 1.0004 - accuracy: 0.5350 - categorical_accuracy: 0.5350 - val_loss: 1.0782 - val_accuracy: 0.5025 - val_categorical_accuracy: 0.5025\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 856s 856ms/step - loss: 1.0076 - accuracy: 0.5465 - categorical_accuracy: 0.5465 - val_loss: 1.0281 - val_accuracy: 0.5025 - val_categorical_accuracy: 0.5025\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 858s 858ms/step - loss: 1.0008 - accuracy: 0.5475 - categorical_accuracy: 0.5475 - val_loss: 1.0273 - val_accuracy: 0.5000 - val_categorical_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "attentive_doc_model = AttentiveDocModel(150, 300, 200, 0.5)\n",
    "attentive_doc_model.compile(optimizer='adam', metrics=['accuracy', 'categorical_accuracy'],\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "attentive_doc_model(next(iter(train_dataset))[0])\n",
    "attentive_doc_model_hist = attentive_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)\n",
    "    \n",
    "with open(f'best_att_doc_model_history.json', 'w') as f:\n",
    "    json.dump(pythonize(attentive_doc_model_hist.history), f)\n",
    "\n",
    "attentive_doc_model.save_weights('./best_att_doc_model/ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallAttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, dropout, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "        \n",
    "        self.dropout.build((BATCH_SIZE, hidden_units))\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        (inputs, sent_mask, para_mask, doc_mask) = inputs\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = array_ops.reshape(\n",
    "            embedded, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH, -1))\n",
    "        sent_mask = array_ops.reshape(\n",
    "            sent_mask, (BATCH_SIZE * MAX_DOC_LENGTH * MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "        sent_embedded, sent_weights = self.sent_encoder(embedded, mask=sent_mask)\n",
    "        sent_embedded = array_ops.reshape(\n",
    "            sent_embedded, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH, -1))\n",
    "        doc_mask = array_ops.reshape(\n",
    "            para_mask, (BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH))\n",
    "        \n",
    "        x, doc_weights = self.doc_encoder(sent_embedded, mask=doc_mask)\n",
    "        x = self.hidden(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if not training:\n",
    "            self.sent_weights = sent_weights\n",
    "            self.doc_weights = doc_weights\n",
    "        \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer small_attentive_doc_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1633s 2s/step - loss: 1.0359 - accuracy: 0.5075 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1625s 2s/step - loss: 1.0250 - accuracy: 0.5115 - val_loss: 1.0298 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1624s 2s/step - loss: 1.0244 - accuracy: 0.5125 - val_loss: 1.0428 - val_accuracy: 0.4975\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1624s 2s/step - loss: 1.0273 - accuracy: 0.5125 - val_loss: 1.0374 - val_accuracy: 0.4975\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1624s 2s/step - loss: 1.0226 - accuracy: 0.5175 - val_loss: 1.0284 - val_accuracy: 0.5150\n",
      "WARNING:tensorflow:Layer small_attentive_doc_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1656s 2s/step - loss: 1.0343 - accuracy: 0.5085 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1650s 2s/step - loss: 1.0290 - accuracy: 0.5085 - val_loss: 1.0379 - val_accuracy: 0.4975\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1647s 2s/step - loss: 1.0293 - accuracy: 0.5095 - val_loss: 1.0473 - val_accuracy: 0.4975\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1646s 2s/step - loss: 1.0251 - accuracy: 0.5120 - val_loss: 1.0392 - val_accuracy: 0.4975\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1647s 2s/step - loss: 1.0293 - accuracy: 0.5155 - val_loss: 1.0361 - val_accuracy: 0.5075\n",
      "WARNING:tensorflow:Layer small_attentive_doc_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1652s 2s/step - loss: 1.0338 - accuracy: 0.5075 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1651s 2s/step - loss: 1.0308 - accuracy: 0.5085 - val_loss: 1.0380 - val_accuracy: 0.4975\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1688s 2s/step - loss: 1.0289 - accuracy: 0.5100 - val_loss: 1.0383 - val_accuracy: 0.4975\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1692s 2s/step - loss: 1.0276 - accuracy: 0.5090 - val_loss: 1.0393 - val_accuracy: 0.4975\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1698s 2s/step - loss: 1.0294 - accuracy: 0.5100 - val_loss: 1.0369 - val_accuracy: 0.4975\n",
      "WARNING:tensorflow:Layer small_attentive_doc_model_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1725s 2s/step - loss: 1.0352 - accuracy: 0.5040 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1704s 2s/step - loss: 1.0269 - accuracy: 0.5090 - val_loss: 1.0449 - val_accuracy: 0.4975\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1708s 2s/step - loss: 1.0269 - accuracy: 0.5090 - val_loss: 1.0388 - val_accuracy: 0.4975\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1718s 2s/step - loss: 1.0285 - accuracy: 0.5090 - val_loss: 1.0415 - val_accuracy: 0.4975\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1709s 2s/step - loss: 1.0269 - accuracy: 0.5090 - val_loss: 1.0382 - val_accuracy: 0.4975\n",
      "Model: \"small_attentive_doc_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  1883200   \n",
      "_________________________________________________________________\n",
      "attentive_sequence_encoder_2 multiple                  331401    \n",
      "_________________________________________________________________\n",
      "attentive_sequence_encoder_3 multiple                  571401    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  90300     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  903       \n",
      "=================================================================\n",
      "Total params: 2,877,205\n",
      "Trainable params: 994,005\n",
      "Non-trainable params: 1,883,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 0.515)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_small_att_doc_model = None\n",
    "best_small_units = None\n",
    "best_small_val_score = 0.0\n",
    "\n",
    "for attention_units in (100, 200, 300, 400):\n",
    "    small_att_doc_model = SmallAttentiveDocModel(150, 300, attention_units, 0.5)\n",
    "    small_att_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                                loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "    small_att_doc_model(next(iter(train_dataset))[0])\n",
    "    small_att_doc_model_hist = small_att_doc_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=5)\n",
    "    \n",
    "    with open(f'small_att_doc_model_history_{attention_units}.json', 'w') as f:\n",
    "        json.dump(pythonize(small_att_doc_model_hist.history), f)\n",
    "        \n",
    "    val_accuracy = max(small_att_doc_model_hist.history['val_accuracy'])\n",
    "    if val_accuracy > best_small_val_score:\n",
    "        best_small_att_doc_model = small_att_doc_model\n",
    "        best_small_units = attention_units\n",
    "        best_small_val_score = val_accuracy\n",
    "\n",
    "best_small_att_doc_model.summary()\n",
    "best_small_units, best_small_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer small_attentive_doc_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1833s 2s/step - loss: 1.0337 - accuracy: 0.5085 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1824s 2s/step - loss: 1.0342 - accuracy: 0.5080 - val_loss: 1.0396 - val_accuracy: 0.4975\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1808s 2s/step - loss: 1.0455 - accuracy: 0.5090 - val_loss: 1.0449 - val_accuracy: 0.5125\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1831s 2s/step - loss: 1.0329 - accuracy: 0.5085 - val_loss: 1.0389 - val_accuracy: 0.5075\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1820s 2s/step - loss: 1.0266 - accuracy: 0.5105 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1827s 2s/step - loss: 1.0294 - accuracy: 0.5110 - val_loss: 1.0391 - val_accuracy: 0.5025\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1825s 2s/step - loss: 1.0225 - accuracy: 0.5100 - val_loss: 1.0386 - val_accuracy: 0.4975\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1839s 2s/step - loss: 1.0229 - accuracy: 0.5115 - val_loss: 1.0414 - val_accuracy: 0.5025\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1838s 2s/step - loss: 1.0167 - accuracy: 0.5120 - val_loss: 1.0420 - val_accuracy: 0.4975\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1816s 2s/step - loss: 1.0120 - accuracy: 0.5180 - val_loss: 1.0312 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "small_att_doc_model = SmallAttentiveDocModel(150, 300, 100, 0.5)\n",
    "small_att_doc_model.compile(optimizer='adam', metrics=['accuracy'],\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "small_att_doc_model(next(iter(train_dataset))[0])\n",
    "small_att_doc_model_hist = small_att_doc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS)\n",
    "    \n",
    "with open(f'best_small_att_doc_model_history.json', 'w') as f:\n",
    "    json.dump(pythonize(small_att_doc_model_hist.history), f)\n",
    "\n",
    "small_att_doc_model.save_weights('./small_att_doc_model/ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
