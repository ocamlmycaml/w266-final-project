{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from gcdc_data import load, load_pandas, Source, TrainOrTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '3' '1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2\n",
       "1  Our friend, General Joe Ballard owns The Raven...     2\n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3\n",
       "3  Responding to separate emails from Uzra + Jeff...     1\n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_pandas(TrainOrTest.TRAIN, sources=[Source.CLINTON])\n",
    "test_data = load_pandas(TrainOrTest.TEST, sources=[Source.CLINTON])\n",
    "\n",
    "print(train_data['label'].unique())\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13163, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='unk')\n",
    "t.fit_on_texts([\n",
    "    word\n",
    "    for doc in train_data['text']\n",
    "    for sent in sent_tokenize(doc)\n",
    "    for word in word_tokenize(sent)\n",
    "])\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "vocab_size, t.word_index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "[-0.45414001 -0.55923003  0.72431999  0.74511999 -0.06304    -0.43191001\n",
      "  0.1787      0.99759001 -0.51115    -0.79369998 -0.22916999 -0.47031999\n",
      "  0.54519999  0.28490999 -0.11225     0.21269    -0.50792003 -0.64273\n",
      " -0.91689003 -1.00320005 -0.41808999 -0.40024    -0.21836001  0.025622\n",
      " -0.56033999 -0.64148998 -0.018587    0.23577     0.24417999  0.53899997\n",
      " -0.74356002 -0.14578     0.24029     0.013151   -0.43454999  0.59173\n",
      "  0.41881001  0.47510999 -0.28112999 -0.10072    -0.98942    -0.49941\n",
      "  1.0158      0.06466    -0.11672    -0.43849    -0.44154    -0.21893001\n",
      " -0.46342999 -0.57498997  0.052368   -0.79330999  0.24862     0.5359\n",
      " -0.60610998 -2.62479997 -0.4858     -0.059422    1.39530003  0.80589998\n",
      " -1.08729994  0.39109999 -0.22679999 -0.75388002  0.0095413  -0.11887\n",
      " -0.20058     1.07710004  0.53029001 -0.47058001  0.31349    -0.97333997\n",
      " -0.49098    -1.37979996 -0.62849998  0.3312     -0.19378    -0.39239001\n",
      " -1.2428     -0.0152      0.24099    -0.15699001  0.059579   -0.46505001\n",
      " -1.37530005  0.017748   -0.035752    0.57725     0.28986999 -0.59584999\n",
      "  0.45642999 -0.34408    -0.36342001  0.38122001 -0.54203999  0.41855001\n",
      "  0.22355001 -0.13675     0.85501999  0.26429999]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('data', f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word, embeddings_index['unk'])\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(t.word_index.keys() - embeddings_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_document = \"\"\"Dear abby,\n",
    "\n",
    "I'm writing to tell you you suck. Help me out of this mess.\n",
    "\n",
    "Bye\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Dear', 'abby', ',']], [['I', \"'m\", 'writing', 'to', 'tell', 'you', 'you', 'suck', '.'], ['Help', 'me', 'out', 'of', 'this', 'mess', '.']], [['Bye']]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, tok=None):\n",
    "    return [\n",
    "        [\n",
    "            tok.texts_to_sequences(nltk.word_tokenize(sent))\n",
    "            if tok else\n",
    "            nltk.word_tokenize(sent)\n",
    "            for sent in nltk.sent_tokenize(para)\n",
    "        ]\n",
    "        for para in text.splitlines()\n",
    "        if len(para) > 0\n",
    "    ]\n",
    "\n",
    "print(tokenize(example_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[163], [1], []]], [[[8], [127], [792], [3], [340], [13], [13], [1], []], [[138], [47], [58], [5], [16], [5058], []]], [[[6344]]]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(example_document, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl:\\n\\nAre we in a good place to begin pap...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[[122], []]], [[[23], [14], [7], [6], [100],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our friend, General Joe Ballard owns The Raven...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[[30], [809], [], [301], [1211], [3403], [71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Outstanding news! Miki Rakic called about 10 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[[1540], [294], []], [[5163], [7116], [203],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Responding to separate emails from Uzra + Jeff...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[[2093], [3], [1348], [1455], [28], [5171], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guy from Mexico is in NY and is cooperating. D...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[[1113], [28], [570], [11], [7], [657], [4],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  Cheryl:\\n\\nAre we in a good place to begin pap...     2   \n",
       "1  Our friend, General Joe Ballard owns The Raven...     2   \n",
       "2  Outstanding news! Miki Rakic called about 10 m...     3   \n",
       "3  Responding to separate emails from Uzra + Jeff...     1   \n",
       "4  Guy from Mexico is in NY and is cooperating. D...     1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [[[[122], []]], [[[23], [14], [7], [6], [100],...  \n",
       "1  [[[[30], [809], [], [301], [1211], [3403], [71...  \n",
       "2  [[[[1540], [294], []], [[5163], [7116], [203],...  \n",
       "3  [[[[2093], [3], [1348], [1455], [28], [5171], ...  \n",
       "4  [[[[1113], [28], [570], [11], [7], [657], [4],...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tokenized'] = train_data['text'].map(lambda text: tokenize(text, t))\n",
    "test_data['tokenized'] = test_data['text'].map(lambda text: tokenize(text, t))\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 19, 137)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_DOC_LENGTH = 0\n",
    "MAX_PARA_LENGTH = 0\n",
    "MAX_SENT_LENGTH = 0\n",
    "\n",
    "for doc in train_data['tokenized'].append(test_data['tokenized']):\n",
    "    MAX_DOC_LENGTH = max(MAX_DOC_LENGTH, len(doc))\n",
    "    for para in doc:\n",
    "        MAX_PARA_LENGTH = max(MAX_PARA_LENGTH, len(para))\n",
    "        for sent in para:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(sent))\n",
    "            \n",
    "MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[122], []]], [[[23], [14], [7], [6], [100], [234], [3], [620], [2593], [3], [469], [2946], [3397], [60], [112], [10], [7106], [7], [600], []]], [[[36], [41], [], [8], [31], [1450], [14], [306], [46], [3], [210], [10], [600], [1776], [4], [8], [17], [77], [2], [3398], [187], [3], [337], [2], [2593], []], [[55], [17], [18], [1659], [3], [48], [12], [1777], [172], [], [63], [17], [3399], [46], [3], [916], [807], [12], [2298], [], [4111], [], [7107], [], [284], [], [4], [16], [1777], [185], [1052], [17], [4112], [46], [178], [2], [245], [5], [2], [556], [10], [881], [516], [], [1923], []], [[16], [359], [31], [222], [47], [280], [172], [59], [41], [5], [2947], [140], [6], [4113], [1149], [12], [3400], [10], [769], [838], []]], [[[8], [66], [27], [2948], [10], [7108], [58], [3], [3400], [], [2], [2086], [14], [77], [2], [52], [44], [11], [3], [18], [191], [], [4], [14], [93], [27], [77], [2594], [], [4114], [58], [51], [140], [14], [1150], [], [27], [6], [7109], [7110], [808], [], [194], [14], [42], [364], [1345], [9], [1924], [12], [1151], [59], [881], [], []], [[26], [], [36], [2], [1273], [23], [7], [234], [], [5158], [4], [8], [31], [81], [3], [170], [58], [1925, 769], [7], [568], [3], [1209], [7], [5159], [331], [10], [769], [1274, 394], []]], [[[56], [3401, 7], [1110], [11], [722], [12], [769], [586], [24], [7111], [10], [2], [3402], [882], []], [[952], [199], [14], [5160], [], [9], [470], [11], [364], [4], [5158], [4], [8], [23], [1210], [12], [2], [185], []]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[122.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 23.,  14.,   7., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 36.,  41.,   8., ...,   0.,   0.,   0.],\n",
       "         [ 55.,  17.,  18., ...,   0.,   0.,   0.],\n",
       "         [ 16., 359.,  31., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0., ...,   0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_to_dense(M, sent_len=MAX_SENT_LENGTH, para_len=MAX_PARA_LENGTH, doc_len=MAX_DOC_LENGTH):\n",
    "    Z = np.zeros((len(M), doc_len, para_len, sent_len))\n",
    "    for docidx, doc in enumerate(M):\n",
    "        for paraidx, para in enumerate(doc):\n",
    "            for sentidx, sent in enumerate(para):\n",
    "                sentnp = np.hstack(np.array(sent))\n",
    "                Z[docidx, paraidx, sentidx, :len(sentnp)] += sentnp\n",
    "    return Z\n",
    "\n",
    "print(train_data['tokenized'][0])\n",
    "pad_to_dense([train_data['tokenized'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1000, 12, 19, 137]), TensorShape([200, 12, 19, 137]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = tf.convert_to_tensor(pad_to_dense(train_data['tokenized']))\n",
    "test_tensor = tf.convert_to_tensor(pad_to_dense(test_data['tokenized']))\n",
    "\n",
    "train_tensor.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1000, 3]), TensorShape([200, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorical_labels(labels):\n",
    "    eye = [\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    for item in labels:\n",
    "        result.append(eye[int(item) - 1])\n",
    "        \n",
    "    return result\n",
    "\n",
    "train_labels = tf.convert_to_tensor(categorical_labels(train_data['label']))\n",
    "test_labels = tf.convert_to_tensor(categorical_labels(test_data['label']))\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 19, 137, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=284, shape=(100,), dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False)\n",
    "\n",
    "embedded_example = embedding(train_tensor[:2])\n",
    "\n",
    "print(embedded_example.shape)\n",
    "embedded_example[0][0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onions\n",
    "\n",
    "class BahdanauAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttentionLayer, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.cached_attention_weights = []\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # (batch_size, ...) -> (batch_size, 1, ...)\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # (batch_size, max_length, 1)\n",
    "        scores = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # (batch_size, max_length, 1) normalized lulz\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "        \n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AttentiveSequenceEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_units, attention_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_state=True)\n",
    "        self.attention = BahdanauAttentionLayer(attention_units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        sequence_encoded, state_h, state_c = self.lstm(inputs)\n",
    "        output, attention_weights = self.attention(sequence_encoded, state_h)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [12, 137]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-177f88c437ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdoc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdoc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_DOC_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_PARA_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdoc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    671\u001b[0m                            'method accepts an `inputs` argument.')\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mchild_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    197\u001b[0m       raise ValueError(\n\u001b[1;32m    198\u001b[0m           \u001b[0;34m'`TimeDistributed` Layer should be passed an `input_shape ` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m           'with at least 3 dimensions, received: ' + str(input_shape))\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;31m# Don't enforce the batch or time dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [12, 137]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2  # keep it low\n",
    "\n",
    "from tensorflow.keras.layers import TimeDistributed, Bidirectional, LSTM, Dense, Dropout, Embedding\n",
    "\n",
    "doc_model = tf.keras.Sequential([\n",
    "    Embedding(\n",
    "        vocab_size,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "        input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH)),\n",
    "    TimeDistributed(TimeDistributed(Bidirectional(LSTM(300)))),\n",
    "    TimeDistributed(Bidirectional(LSTM(300))),\n",
    "    Bidirectional(LSTM(300)),\n",
    "    Dense(100),\n",
    "    Dropout(0.5),\n",
    "    Dense(3)\n",
    "])\n",
    "\n",
    "\n",
    "doc_model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "doc_model.build((MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model.fit(\n",
    "    train_tensor, train_labels,\n",
    "    batch_size=BATCH_SIZE, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_model.evaluate(test_tensor, test_labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            mask_zero=True,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.flatten = FlattenSequence()\n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.para_shaper = ExpandSequence((BATCH_SIZE * MAX_DOC_LENGTH, MAX_PARA_LENGTH))\n",
    "        self.para_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_shaper = ExpandSequence((BATCH_SIZE, MAX_DOC_LENGTH))\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x, sent_weights = self.sent_encoder(x)\n",
    "        x = self.para_shaper(x)\n",
    "        x, para_weights = self.para_encoder(x)\n",
    "        x = self.doc_shaper(x)\n",
    "        x, doc_weights = self.doc_encoder(x)\n",
    "        \n",
    "        self.sent_weights = sent_weights\n",
    "        self.para_weights = para_weights\n",
    "        self.doc_weights = doc_weights\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "attentive_doc_model = AttentiveDocModel(100, 100, 50)\n",
    "attentive_doc_model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "attentive_doc_model.build((BATCH_SIZE, MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "attentive_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentive_doc_model.fit(\n",
    "    train_tensor, train_labels,\n",
    "    batch_size=BATCH_SIZE) #, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentive_doc_model.evaluate(test_tensor, test_labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerAttentiveDocModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units, hidden_units, attention_units, batch_size=BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "            mask_zero=True,\n",
    "            input_shape=(MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "        \n",
    "        self.flatten = FlattenSequence()\n",
    "        self.sent_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        self.doc_shaper = ExpandSequence((BATCH_SIZE, MAX_DOC_LENGTH * MAX_PARA_LENGTH))\n",
    "        self.doc_encoder = AttentiveSequenceEncoder(lstm_units, attention_units)\n",
    "        \n",
    "        self.hidden = tf.keras.layers.Dense(hidden_units, activation='tanh')\n",
    "        self.classifier = tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x, sent_weights = self.sent_encoder(x)\n",
    "        x = self.doc_shaper(x)\n",
    "        x, doc_weights = self.doc_encoder(x)\n",
    "        \n",
    "        self.sent_weights = sent_weights\n",
    "        self.doc_weights = doc_weights\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "smaller_attentive_doc_model = SmallerAttentiveDocModel(100, 100, 50)\n",
    "smaller_attentive_doc_model.compile(optimizer='adam', metrics=['accuracy'], loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "smaller_attentive_doc_model.build((BATCH_SIZE, MAX_DOC_LENGTH, MAX_PARA_LENGTH, MAX_SENT_LENGTH))\n",
    "\n",
    "smaller_attentive_doc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_attentive_doc_model.fit(\n",
    "    train_tensor, train_labels,\n",
    "    batch_size=BATCH_SIZE) #, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_attentive_doc_model.evaluate(test_tensor, test_labels, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
